{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedron18/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DecisionTransformerModel, DecisionTransformerConfig\n",
    "from time import sleep\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model: 12.26 MB (0.01 GB)\n"
     ]
    }
   ],
   "source": [
    "max_ep_length = 11264 # maximum number that can exists in timesteps (frame number we skip 10 so this gets high)\n",
    "n_positions = 2**10 # The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n",
    "batch_size = 64 # The batch size to use for training.\n",
    "state_dim = (10,10,10)\n",
    "state_dim_flatten = state_dim[0]*state_dim[1]*state_dim[2]\n",
    "action_dim = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=state_dim_flatten, act_dim=action_dim, max_ep_len=max_ep_length, n_positions=n_positions)\n",
    "model = DecisionTransformerModel(config).to(device=device)\n",
    "\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_model_mb = (param_size + buffer_size) / 1024**2\n",
    "size_model_gb = size_model_mb / 1024\n",
    "print(f\"Size of model: {size_model_mb:.2f} MB ({size_model_gb:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: torch.Size([2, 1000]), actions: torch.Size([2, 6]), target_return: torch.Size([2, 1]), timesteps: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# fake data\n",
    "\n",
    "def fake_data(sequence_length):\n",
    "    target_return = torch.rand((sequence_length, 1), device=device, dtype=torch.float32) # expected future return\n",
    "    states = torch.rand((sequence_length, state_dim_flatten), device=device, dtype=torch.float32)#.reshape(1, 1, state_dim_flatten) #\n",
    "    actions = torch.rand((sequence_length, action_dim), device=device, dtype=torch.float32)\n",
    "    timesteps = torch.tensor([i for i in range(1, sequence_length+1)], device=device, dtype=torch.long) # integer what timestep we on\n",
    "\n",
    "    return states, actions, target_return, timesteps\n",
    "\n",
    "\n",
    "states, actions, target_return, timesteps = fake_data(2)\n",
    "\n",
    "print(f\"states: {states.shape}, actions: {actions.shape}, target_return: {target_return.shape}, timesteps: {timesteps.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
